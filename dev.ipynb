{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/1L-Sparse-Autoencoder/blob/main/dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *ğŸ“– Dictionary Learning, ğŸ•µ Interpretability, ğŸŒœ ğŸŒ› and Supersymmetry*"
      ],
      "metadata": {
        "id": "3rhxTT4SxyPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook documents a personal exploration into intriguing parallels between the fields of **Deep Learning** and **Supersymmetry**. Recent developments in deep learning, specifically in the area of **mechanistic interpretability**, mirror theoretical constructs in **supersymmetry** (SUSY), suggesting that mathematical frameworks used to describe particle physics might enrich our understanding of neural networks.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "In the realms of theoretical physics and artificial intelligence, complex systems are often broken down through similar mathematical lensesâ€”albeit with different end goals. This project hypothesizes that the methods used to dissect and interpret neural networks in deep learning could benefit from supersymmetric theories, particularly through the lens of **holoraumy tensors**.\n",
        "\n",
        "**Supersymmetry** posits a correspondence between two basic classes of particles: bosons and fermions. Holoraumy tensors, as discussed in this [paper](https://arxiv.org/abs/1906.02971), provide a framework for understanding transformations within supersymmetrical systems. These tensors describe the electromagnetic-duality rotations that are crucial for linking various properties of supermultiplets in four-dimensional, N=1 supersymmetry.\n",
        "\n",
        "\n",
        "**Mechanistic Interpretability** (MI) focuses on understanding the precise mechanisms through which neural networks process inputs to produce outputs. This often involves the manipulation of high-dimensional spaces to extract meaningful patternsâ€”akin to transforming and reducing dimensions in physical theories.\n",
        "\n",
        "Recently, I've noticed that the advancements in interpretability, particularly those by [Chris Olah](https://scholar.google.com/citations?user=6dskOSUAAAAJ&hl=en) at [Anthropic](https://www.anthropic.com/research) on [Transformer Circuits](https://transformer-circuits.pub/), closely parallel the concepts I've worked with on the paper linked above as part of research group led by [Dr. Sylvester James Gates (Jim)](https://twitter.com/dr_jimgates) at Brown University.\n",
        "\n",
        "\n",
        "\n",
        "These observations suggest that the mathematical principles of supersymmetry and quantum field theory could directly apply to transformer-based neural networks, potentially deepening our understanding of model mechanics and guiding models to align more closely with human interests through techniques like ablation and masking.\n",
        "\n",
        "## Hypothesis and Objectives\n",
        "\n",
        "**Hypothesis**: The electromagnetic-duality rotations and the structures of holoraumy tensors in supersymmetry can analogously describe transformations within deep learning models, particularly in how neural networks achieve dimensionality reduction and feature extraction.\n",
        "\n",
        "**Objectives**:\n",
        "1. **Define and Formalize**: Construct a detailed mathematical analogy between the operations in neural networks and supersymmetric transformations, focusing on the role of holoraumy tensors in describing system dynamics.\n",
        "2. **Analogize and Model**: Develop a model to demonstrate how supersymmetric principles, particularly those involving dimensional transformations and symmetry, could theoretically underpin neural network operations.\n",
        "\n",
        "## Definitions and Theoretical Background\n",
        "### 1. Mechanistic Interpretability in Deep Learning\n",
        "- #### 1.1 **Dictionary Learning**\n",
        "A recent technique termed **dictionary learning** by the team at Anthropic involves dimensionality manipulation where inputs are projected into a high-dimensional space to capture latent features:\n",
        "$$\n",
        "  \\mathbf{z} = f_{\\text{encode}}(\\mathbf{x}; \\theta_e), \\\\\n",
        "$$\n",
        "$$\n",
        "\\mathbf{y} = f_{\\text{decode}}(\\mathbf{z}; \\theta_d)\n",
        "$$\n",
        "Here, $\\mathbf{x}$ represents the input, $\\mathbf{z}$ the encoded latent space, and $\\mathbf{y}$ the output, with $\\theta_e$ and $\\theta_d$ as the encoder and decoder parameters, respectively.\n",
        "\n",
        "### 2. Supersymmetry and Holoraumy Tensors\n",
        "- #### 2.1 - Supersymmetry\n",
        "Supersymmetry posits a fundamental symmetry between bosons and fermions, often involving dimensional transformations to relate these particle types through algebraic structures:\n",
        "$$\n",
        "Q | \\text{Boson} \\rangle = | \\text{Fermion} \\rangle, \\quad Q | \\text{Fermion} \\rangle = | \\text{Boson} \\rangle\n",
        "$$\n",
        "Where \\(Q\\) is the supersymmetry generator.\n",
        "\n",
        "\n",
        "## Exploratory Framework\n",
        "\n",
        "This research will unfold through several structured phases:\n",
        "1. **Review of Supersymmetric Theories**: Summarize key supersymmetric theories, especially focusing on the role and computation of holoraumy tensors in 4D, N=1 supersymmetry as detailed in the aforementioned paper.\n",
        "2. **Application to Neural Networks**: Propose and formulate how these supersymmetric concepts could map onto neural network operations, particularly in interpretability and model simplification.\n",
        "3. **Empirical Analysis**: Develop simulations or theoretical models to test these analogies, assessing their utility in providing new insights into neural network behavior.\n",
        "4. **Validation**: Critically evaluate whether these supersymmetric approaches can be effectively integrated into current deep learning frameworks, offering tangible benefits.\n",
        "\n",
        "#### Summary and Next Steps\n",
        "\n",
        "This notebook aims to rigorously explore the potential foundational link between deep learning and supersymmetry, adhering to strict scientific principles throughout. Our objective is to substantiate or refute our hypothesis with robust theoretical backing and empirical evidence.\n",
        "\n",
        "The initial step is to replicate the results from the [paper](https://transformer-circuits.pub/2023/monosemantic-features) by Bricken et al. from Anthropic."
      ],
      "metadata": {
        "id": "SjBHiDo0TUMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. `Setup`"
      ],
      "metadata": {
        "id": "x0WX6lN2guwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 - `Setup` - Install Dependencies"
      ],
      "metadata": {
        "id": "Ao7MchHThtLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qzU1eYv0SvAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d3da92-ae07-4d52-e2bc-6f8deac6f1db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python3 -V"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformer_lens\n",
        "!pip install gradio\n",
        "!pip install tiktoken\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "lbFhN8dWTK_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5baec1e-17a5-4165-d9da-ad4fb77a40ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformer_lens\n",
            "  Downloading transformer_lens-1.17.0-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.1/137.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.23.0 (from transformer_lens)\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting better-abc<0.0.4,>=0.0.3 (from transformer_lens)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Collecting datasets>=2.7.1 (from transformer_lens)\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops>=0.6.0 (from transformer_lens)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
            "  Downloading jaxtyping-0.2.28-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.0.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.7.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.2)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.40.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.11.0)\n",
            "Collecting wandb>=0.13.5 (from transformer_lens)\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.13.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.31.0)\n",
            "Collecting xxhash (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.9.5)\n",
            "Collecting huggingface-hub (from accelerate>=0.23.0->transformer_lens)\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typeguard==2.13.3 (from jaxtyping>=0.2.11->transformer_lens)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->transformer_lens)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading sentry_sdk-2.0.1-py2.py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: better-abc, xxhash, typeguard, smmap, setproctitle, sentry-sdk, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fancy-einsum, einops, docker-pycreds, dill, beartype, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jaxtyping, huggingface-hub, gitdb, nvidia-cusolver-cu12, GitPython, wandb, datasets, accelerate, transformer_lens\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed GitPython-3.1.43 accelerate-0.29.3 beartype-0.14.1 better-abc-0.0.3 datasets-2.19.0 dill-0.3.8 docker-pycreds-0.4.0 einops-0.8.0 fancy-einsum-0.0.3 gitdb-4.0.11 huggingface-hub-0.22.2 jaxtyping-0.2.28 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentry-sdk-2.0.1 setproctitle-1.3.3 smmap-5.0.1 transformer_lens-1.17.0 typeguard-2.13.3 wandb-0.16.6 xxhash-3.4.1\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.28.3-py3-none-any.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.16.0 (from gradio)\n",
            "  Downloading gradio_client-0.16.0-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m314.4/314.4 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.22.2)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.4.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.16.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=a4b3a50783ba66e9bc72ccaa40b58bee3b535bfbfc8094e5ba753979fd10103f\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, aiofiles, uvicorn, starlette, httpcore, typer, httpx, fastapi, gradio-client, gradio\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.110.3 ffmpy-0.3.2 gradio-4.28.3 gradio-client-0.16.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 orjson-3.10.2 pydub-0.25.1 python-multipart-0.0.9 ruff-0.4.2 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 typer-0.12.3 uvicorn-0.29.0 websockets-11.0.3\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lie9Jrw4ezOZ"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vmc78_hXezOZ"
      },
      "outputs": [],
      "source": [
        "# Log in to your W&B account\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MVdXxrUoezOZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c81990fd-efaf-4f85-b6b0-fb14b71c5483"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 - `Setup` - Imports"
      ],
      "metadata": {
        "id": "l1T8XDeQT2QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import gradio as gr\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import plotly.express as px\n",
        "import pprint\n",
        "import pandas as pd\n",
        "import requests\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformer_lens\n",
        "\n",
        "from dataclasses import dataclass, field, fields\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfApi\n",
        "from IPython.display import HTML\n",
        "from functools import partial\n",
        "from transformers import GPT2TokenizerFast\n",
        "from tqdm.auto import tqdm\n",
        "from transformer_lens import (\n",
        "    HookedTransformer,\n",
        "    utils\n",
        ")\n",
        "from typing import Any, Dict\n",
        "\n",
        "\n",
        "seed = 42069\n",
        "DTYPES = {\"float32\": torch.float32, \"float16\": torch.float16, \"bfloat16\": torch.bfloat16}\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset_name = \"tinyshakespeare\"\n",
        "data_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "data_cache_dir = \"data\""
      ],
      "metadata": {
        "id": "dUf6nhkuTggs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 `Setup` - Config definition and initialization"
      ],
      "metadata": {
        "id": "Bt40upYG7sjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "    name: str\n",
        "    batch_size: int\n",
        "    learning_rate: float\n",
        "    dtype: Any\n",
        "    seed: Any\n",
        "\n",
        "@dataclass\n",
        "class TransformerConfig(ModelConfig):\n",
        "    max_length: int\n",
        "    vocabulary_size: int\n",
        "    num_layers: int\n",
        "    num_heads: int\n",
        "    embed_dim: int\n",
        "    hidden_dim: int\n",
        "    nonlinearity: Any\n",
        "\n",
        "@dataclass\n",
        "class AutoEncoderConfig(ModelConfig):\n",
        "    batch_size: int\n",
        "    model_batch_size: int\n",
        "    buffer_mult: int\n",
        "    num_tokens: int\n",
        "    l1_coeff: float\n",
        "    beta1: float\n",
        "    beta2: float\n",
        "    dict_mult: float\n",
        "    seq_len: int\n",
        "    d_mlp: int\n",
        "    enc_dtype: Any\n",
        "    remove_rare_dir: bool\n",
        "    buffer_size: Any\n",
        "    buffer_batches: int\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    name: str\n",
        "    data: Any\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    train_steps: int\n",
        "    eval_steps: int\n",
        "    train_steps_per_eval: int\n",
        "    transformer_learning_rate: float\n",
        "    autoencoder_learning_rate: float\n",
        "    optimizer: Any\n",
        "    transformer_batch_size: int\n",
        "    autoencoder_batch_size: int\n",
        "    device: Any\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    transformer: Any\n",
        "    autoencoder: Any\n",
        "    data: Any\n",
        "    training: Any\n",
        "\n",
        "def download_file(url: str, fname: str, chunk_size=1024):\n",
        "    \"\"\"Helper function to download a file from a given url\"\"\"\n",
        "    resp = requests.get(url, stream=True)\n",
        "    total = int(resp.headers.get(\"content-length\", 0))\n",
        "    with open(fname, \"wb\") as file, tqdm(\n",
        "        desc=fname,\n",
        "        total=total,\n",
        "        unit=\"iB\",\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024,\n",
        "    ) as bar:\n",
        "        for data in resp.iter_content(chunk_size=chunk_size):\n",
        "            size = file.write(data)\n",
        "            bar.update(size)\n",
        "\n",
        "def download_tinyshakespeare(\n",
        "    data_url=data_url,\n",
        "    cache_dir=data_cache_dir\n",
        ") -> tuple:\n",
        "    \"\"\"Downloads the TinyShakespeare dataset to DATA_CACHE_DIR\"\"\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    data_filename = os.path.join(cache_dir, \"tiny_shakespeare.txt\")\n",
        "    if not os.path.exists(data_filename):\n",
        "        print(f\"Downloading {data_url} to {data_filename}...\")\n",
        "        download_file(data_url, data_filename)\n",
        "    else:\n",
        "        print(f\"{data_filename} already exists, skipping download...\")\n",
        "\n",
        "    return data_filename\n",
        "\n",
        "def tokenize_tinyshakespeare(tokenizer, raw_data_filepath, cache_dir=data_cache_dir):\n",
        "    encode = lambda s: tokenizer.encode(s, allowed_special={'<|endoftext|>'})\n",
        "    eot = tokenizer._special_tokens['<|endoftext|>'] # end of text token\n",
        "    data_filename = os.path.join(cache_dir, \"tiny_shakespeare.txt\")\n",
        "    text = open(data_filename, 'r').read()\n",
        "\n",
        "    # let's treat every person's statement in the dialog as a separate document\n",
        "    text = \"<|endoftext|>\" + text\n",
        "    text = text.replace('\\n\\n', '\\n\\n<|endoftext|>')\n",
        "\n",
        "    # encode the text\n",
        "    tokens = encode(text)\n",
        "    tokens_np = np.array(tokens, dtype=np.int32)\n",
        "\n",
        "    # let's take the first 32,768 tokens as the validation split (~10%)\n",
        "    val_tokens_np = tokens_np[:32768]\n",
        "    train_tokens_np = tokens_np[32768:]\n",
        "\n",
        "    # save to file\n",
        "    # val_filename = os.path.join(DATA_CACHE_DIR, \"tiny_shakespeare_val.bin\")\n",
        "    # train_filename = os.path.join(DATA_CACHE_DIR, \"tiny_shakespeare_train.bin\")\n",
        "    # with open(val_filename, \"wb\") as f:\n",
        "    #     f.write(val_tokens_np.tobytes())\n",
        "    # with open(train_filename, \"wb\") as f:\n",
        "    #     f.write(train_tokens_np.tobytes())\n",
        "    # prints\n",
        "    #print(f\"Saved {len(val_tokens_np)} tokens to {val_filename}\")\n",
        "    #print(f\"Saved {len(train_tokens_np)} tokens to {train_filename}\")\n",
        "\n",
        "    return {\"train\": train_tokens_np, \"val\": val_tokens_np}\n",
        "\n",
        "\n",
        "# Data and Tokenizer Configurations\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "raw_data_filepath = download_tinyshakespeare()\n",
        "data = tokenize_tinyshakespeare(tokenizer, raw_data_filepath)\n",
        "# tokenized_data = utils.tokenize_and_concatenate(data, tokenizer, max_length=512)\n",
        "# tokenized_data = tokenized_data.shuffle(42)\n",
        "# all_tokens = tokenized_data[\"tokens\"]\n",
        "\n",
        "# Training Configuration\n",
        "transformer_batch_size = 1\n",
        "autoencoder_batch_size = 4096  # tokens?\n",
        "training_steps = 1e5\n",
        "eval_steps = 5e3\n",
        "train_steps_per_eval = training_steps // 10\n",
        "transformer_learning_rate = 1e-5\n",
        "autoencoder_learning_rate = 1e-4\n",
        "\n",
        "# Model Configurations\n",
        "# Transformer Configuration\n",
        "max_length = 2**7  # 2**7 = 128\n",
        "vocab_size = 50257 # len(tokenizer)  # <s>I think this is the GPT-2 vocab size, or close to it</s>\n",
        "embed_dim = max_length * 4  # Setting the embedding dimension to just be 4x the max length, seems like a relatively reasonable starting point, but I could be wrong...\n",
        "hidden_dim = 1024  # Can't remember if this is actually needed...\n",
        "num_layers = 2  # Trying to reproduce results from https://transformer-circuits.pub/2023/monosemantic-features/index.html\n",
        "block_size = 2**3  # 8\n",
        "num_heads = max_length % block_size  # I believe this will make it so each head (except the last one) computes the attention matrix across blocks of 8 tokens\n",
        "transformer_config = {\n",
        "    \"name\": \"transformer\",\n",
        "    \"batch_size\": transformer_batch_size,\n",
        "    \"learning_rate\": transformer_learning_rate,\n",
        "    \"dtype\": DTYPES[\"bfloat16\"],\n",
        "    \"seed\": seed,\n",
        "    \"max_length\": max_length,\n",
        "    \"vocabulary_size\": vocab_size,\n",
        "    \"embed_dim\": embed_dim,\n",
        "    \"hidden_dim\": hidden_dim,\n",
        "    \"num_layers\": num_layers,\n",
        "    \"num_heads\": num_heads,\n",
        "    \"nonlinearity\": nn.ReLU,  # Will change this to gelu/other later.\n",
        "    \"learning_rate\": transformer_learning_rate\n",
        "}\n",
        "transformer_config = TransformerConfig(**transformer_config)\n",
        "\n",
        "# Autoencoder Configuration\n",
        "model_batch_size = transformer_config.batch_size\n",
        "buffer_mult = 384  #??\n",
        "num_tokens = int(2e9)  #??\n",
        "l1_coeff = 3e-4\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "dict_mult = 8  # <- scaling factor for dictionary learning\n",
        "seq_len = max_length\n",
        "d_mlp = transformer_config.hidden_dim  # 2048\n",
        "enc_dtype = transformer_config.dtype  # \"fp32\"\n",
        "remove_rare_dir = False\n",
        "autoencoder_config = {\n",
        "    \"name\": \"autoencoder\",\n",
        "    \"batch_size\": autoencoder_batch_size,\n",
        "    \"model_batch_size\": model_batch_size,\n",
        "    \"dtype\": DTYPES[\"bfloat16\"],\n",
        "    \"seed\": seed,\n",
        "    \"learning_rate\": autoencoder_learning_rate,\n",
        "    \"buffer_mult\": buffer_mult,\n",
        "    \"num_tokens\": num_tokens,\n",
        "    \"l1_coeff\": l1_coeff,\n",
        "    \"beta1\": beta1,\n",
        "    \"beta2\": beta2,\n",
        "    \"dict_mult\": dict_mult,\n",
        "    \"seq_len\": seq_len,\n",
        "    \"d_mlp\": d_mlp,\n",
        "    \"enc_dtype\": enc_dtype,\n",
        "    \"remove_rare_dir\": remove_rare_dir,\n",
        "\n",
        "}\n",
        "autoencoder_config[\"model_batch_size\"] = 64\n",
        "autoencoder_config[\"buffer_size\"] = autoencoder_config[\"batch_size\"] * autoencoder_config[\"buffer_mult\"]\n",
        "autoencoder_config[\"buffer_batches\"] = autoencoder_config[\"buffer_size\"] // autoencoder_config[\"seq_len\"]\n",
        "autoencoder_config = AutoEncoderConfig(**autoencoder_config)\n",
        "\n",
        "# Configuration initialization.\n",
        "config = Config(\n",
        "    transformer=transformer_config,\n",
        "    autoencoder=autoencoder_config,\n",
        "    data=DataConfig(\n",
        "        **{\n",
        "            \"name\": dataset_name,\n",
        "            \"data\": data\n",
        "          }\n",
        "    ),\n",
        "    training=TrainingConfig(\n",
        "        **{\n",
        "            \"transformer_batch_size\": transformer_batch_size,\n",
        "            \"autoencoder_batch_size\": autoencoder_batch_size,\n",
        "            \"train_steps\": training_steps,\n",
        "            \"eval_steps\": eval_steps,\n",
        "            \"train_steps_per_eval\": train_steps_per_eval,\n",
        "            \"transformer_learning_rate\": transformer_learning_rate,\n",
        "            \"autoencoder_learning_rate\": autoencoder_learning_rate,\n",
        "            \"optimizer\": None,\n",
        "            \"device\": device\n",
        "          }\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "KAqaIhlF7sGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "868adb6b-c463-4f75-c52d-2b46c361e91c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/tiny_shakespeare.txt already exists, skipping download...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 - `Models` - Define the Models"
      ],
      "metadata": {
        "id": "y8ZmfyVBUWCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1.4.1 - Autoencoder\n",
        "$$\n",
        "\\mathbf{z} = f_{\\text{encode}}(\\mathbf{x}; \\theta_e), \\\\\n",
        "\\mathbf{y} = f_{\\text{decode}}(\\mathbf{z}; \\theta_d)\n",
        "$$\n",
        "Here, $\\mathbf{x}$ represents the input, $\\mathbf{z}$ the encoded latent space, and $\\mathbf{y}$ the output, with $\\theta_e$ and $\\theta_d$ as the encoder and decoder parameters, respectively."
      ],
      "metadata": {
        "id": "R3msnJbzT4Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # Encoder that projects to a higher-dimensional space\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)  # Optional: Dropout for regularization\n",
        "        )\n",
        "        # Decoder that projects back to the original space\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded\n"
      ],
      "metadata": {
        "id": "S3ypesK0T67l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.2 - Transformer\n",
        "\n",
        "The self-attention mechanism in the transformer model can be mathematically represented as follows:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "where $Q$, $K$, and $V$ are queries, keys, and values respectively, and $d_k$ is the dimensionality of the keys.\n"
      ],
      "metadata": {
        "id": "zJUE_j_Tg0VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.scale = embed_dim ** -0.5  # Scaling factor for attention scores\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        Q = self.query(query)\n",
        "        K = self.key(key)\n",
        "        V = self.value(value)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attention = self.softmax(scores)\n",
        "        output = torch.matmul(attention, V)\n",
        "        return output\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig, **kwargs):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        if kwargs.get(\"type\", \"attn\")==\"atn\":\n",
        "            self.fc = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        else:\n",
        "            self.fc = nn.Linear(config.hidden_dim, config.vocabulary_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class MHSA(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([\n",
        "            Attention(config.embed_dim, config.num_heads) for _ in range(config.num_heads)\n",
        "        ])\n",
        "        self.linear = nn.Linear(config.num_heads * config.embed_dim, config.embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        head_outputs = [head(x, x, x, mask) for head in self.heads]\n",
        "        concatenated = torch.cat(head_outputs, dim=-1)\n",
        "        return self.linear(concatenated)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.attention = MHSA(config)\n",
        "        self.mlp = MLP(config)\n",
        "        self.norm1 = nn.LayerNorm(config.embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(config.embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x2 = self.norm1(x)\n",
        "        x = x + self.attention(x2, mask)\n",
        "        x2 = self.norm2(x)\n",
        "        x = x + self.mlp(x2)\n",
        "        return x\n",
        "\n",
        "# - [ ] TODO: DEBUG and check for consistency with original work\n",
        "# - [ ] TODO: Add \"ports\" for the autoencoder to hook into\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig, autoencoder: AutoEncoder):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=config.vocabulary_size, embedding_dim=config.embed_dim)\n",
        "        self.positional_encodings = nn.Parameter(torch.zeros(1, config.max_length, config.embed_dim))\n",
        "        self.autoencoder = autoencoder\n",
        "        self.layers = nn.ModuleList([TransformerBlock(config) for _ in range(config.num_layers)])\n",
        "        self.final_layer = nn.Linear(config.embed_dim, config.vocabulary_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x) + self.positional_encodings[:, :x.size(1), :]\n",
        "        encoded, _ = self.autoencoder(x)  # You can choose to use or discard the decoded output\n",
        "        for layer in self.layers:\n",
        "            encoded = layer(encoded, mask)\n",
        "        return self.final_layer(encoded)"
      ],
      "metadata": {
        "id": "p5kuP89Cg3Bw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 - `Function Defs` - Utils\n"
      ],
      "metadata": {
        "id": "kju5GpaKVI80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.1 - `Function Defs` - \"Standard Lib\""
      ],
      "metadata": {
        "id": "copY5zhlgb8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Function Def` - Train the Language Model"
      ],
      "metadata": {
        "id": "EHGizhCXAonF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, tokenizer, autoencoder, data, config: TrainingConfig|None=None, **kwargs):\n",
        "    device = config.device if config else kwargs.get(\"device\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Data split\n",
        "    split1, split2 = round(0.8 * len(data)), round(0.9 * len(data))\n",
        "    train_data, val_data, test_data = data[:split1], data[split1:split2], data[split2:]\n",
        "\n",
        "    # Configuration parameters\n",
        "    eval_every_n_batches = config.train_steps_per_eval or kwargs.get(\"eval_every_n_batches\", 10000)\n",
        "    max_steps = kwargs.get(\"num_steps\", 100000)\n",
        "    optimizer = kwargs.get(\"optimizer\", torch.optim.Adam(model.parameters(), lr=config.transformer_learning_rate if config else 1e-5))\n",
        "    loss_fn = kwargs.get(\"loss_fn\", torch.nn.CrossEntropyLoss())\n",
        "\n",
        "    model.train()\n",
        "    for i in tqdm(range(int(max_steps)), desc=\"Running training loop...\"):\n",
        "        batch_idx = i % len(train_data)\n",
        "        train_example = train_data[batch_idx]\n",
        "\n",
        "        # model_input = tokenizer.encode(train_example[\"text\"], padding=\"max_length\", truncate=True, max_length=model.config.max_length, return_tensors=\"pt\").to(device)\n",
        "        model_input\n",
        "        labels = model_input.clone()  # Assuming a model that outputs logits with the same shape as input\n",
        "        model_output = model(model_input)\n",
        "\n",
        "        loss = loss_fn(model_output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if (i + 1) % eval_every_n_batches == 0:\n",
        "            model.eval()\n",
        "            validation_loss = []\n",
        "            for val_example in val_data:\n",
        "                val_input = tokenizer.encode(val_example[\"text\"], padding=\"max_length\", truncate=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "                with torch.no_grad():\n",
        "                    val_output = model(val_input)\n",
        "                    val_loss = loss_fn(val_output, val_input.clone())\n",
        "                    validation_loss.append(val_loss.item())\n",
        "            avg_val_loss = sum(validation_loss) / len(validation_loss)\n",
        "            print(f\"Validation Loss at step {i+1}: {avg_val_loss}\")\n",
        "            model.train()\n",
        "\n",
        "        if i >= max_steps:\n",
        "            break\n",
        "\n",
        "    # Testing loop or additional analysis could be added here\n",
        "    return model, {'training_loss': loss.item(), 'validation_loss': avg_val_loss}, test_data\n"
      ],
      "metadata": {
        "id": "lLKz3olJAtHW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Function Def` - Evaluate the Language Model"
      ],
      "metadata": {
        "id": "GOd-KyMJ6-vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, data):\n",
        "    ..."
      ],
      "metadata": {
        "id": "pGLnzmjb6_OG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.2 - `Function Defs` - Mechanistic Interpretibility"
      ],
      "metadata": {
        "id": "VfFLR5cnghBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.2.1 - `Function Defs` - Get Reconstruction Loss"
      ],
      "metadata": {
        "id": "3jR9xXgyVbih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replacement_hook(mlp_post, hook, encoder):\n",
        "    mlp_post_reconstr = encoder(mlp_post)[1]\n",
        "    return mlp_post_reconstr\n",
        "\n",
        "def mean_ablate_hook(mlp_post, hook):\n",
        "    mlp_post[:] = mlp_post.mean([0, 1])\n",
        "    return mlp_post\n",
        "\n",
        "def zero_ablate_hook(mlp_post, hook):\n",
        "    mlp_post[:] = 0.\n",
        "    return mlp_post\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_recons_loss(model, all_tokens, num_batches, local_encoder):\n",
        "    loss_list = []\n",
        "    for i in range(num_batches):\n",
        "        tokens = all_tokens[torch.randperm(len(all_tokens))[model.config.batch_size]]\n",
        "        loss = model(tokens, return_type=\"loss\")\n",
        "        recons_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"post\", 0), partial(replacement_hook, encoder=local_encoder))])\n",
        "        # mean_abl_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"post\", 0), mean_ablate_hook)])\n",
        "        zero_abl_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"post\", 0), zero_ablate_hook)])\n",
        "        loss_list.append((loss, recons_loss, zero_abl_loss))\n",
        "    losses = torch.tensor(loss_list)\n",
        "    loss, recons_loss, zero_abl_loss = losses.mean(0).tolist()\n",
        "\n",
        "    print(f\"loss: {loss:.4f}, recons_loss: {recons_loss:.4f}, zero_abl_loss: {zero_abl_loss:.4f}\")\n",
        "    score = ((zero_abl_loss - recons_loss)/(zero_abl_loss - loss))\n",
        "    print(f\"Reconstruction Score: {score:.2%}\")\n",
        "    # print(f\"{((zero_abl_loss - mean_abl_loss)/(zero_abl_loss - loss)).item():.2%}\")\n",
        "    return score, loss, recons_loss, zero_abl_loss"
      ],
      "metadata": {
        "id": "E-XN_do8VabW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.2.2 - `Function Defs` - Get Frequencies"
      ],
      "metadata": {
        "id": "zxzbXJvLVhV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency\n",
        "@torch.no_grad()\n",
        "def get_freqs(model, all_tokens, num_batches, local_encoder, config: AutoEncoderConfig):\n",
        "    act_freq_scores = torch.zeros(local_encoder.d_hidden, dtype=torch.float32).cuda()\n",
        "    total = 0\n",
        "    for i in tqdm(num_batches):\n",
        "        tokens = all_tokens[torch.randperm(len(all_tokens))[:config.batch_size]]\n",
        "\n",
        "        _, cache = model.run_with_cache(tokens, stop_at_layer=1, names_filter=utils.get_act_name(\"post\", 0))\n",
        "        mlp_acts = cache[utils.get_act_name(\"post\", 0)]\n",
        "        mlp_acts = mlp_acts.reshape(-1, d_mlp)\n",
        "\n",
        "        hidden = local_encoder(mlp_acts)[2]\n",
        "\n",
        "        act_freq_scores += (hidden > 0).sum(0)\n",
        "        total+=hidden.shape[0]\n",
        "    act_freq_scores /= total\n",
        "    num_dead = (act_freq_scores==0).float().mean()\n",
        "    print(\"Num dead\", num_dead)\n",
        "    return act_freq_scores"
      ],
      "metadata": {
        "id": "y_UReEIDVjzo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.2.3 - `Functions Defs` - Visualise Feature Utils"
      ],
      "metadata": {
        "id": "IAr8Tr8sVn-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from html import escape\n",
        "import colorsys\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "SPACE = \"Â·\"\n",
        "NEWLINE=\"â†©\"\n",
        "TAB = \"â†’\"\n",
        "\n",
        "def create_html(strings, values, max_value=None, saturation=0.5, allow_different_length=False, return_string=False):\n",
        "    # escape strings to deal with tabs, newlines, etc.\n",
        "    escaped_strings = [escape(s, quote=True) for s in strings]\n",
        "    processed_strings = [\n",
        "        s.replace(\"\\n\", f\"{NEWLINE}<br/>\").replace(\"\\t\", f\"{TAB}&emsp;\").replace(\" \", \"&nbsp;\")\n",
        "        for s in escaped_strings\n",
        "    ]\n",
        "\n",
        "    if isinstance(values, torch.Tensor) and len(values.shape)>1:\n",
        "        values = values.flatten().tolist()\n",
        "\n",
        "    if not allow_different_length:\n",
        "        assert len(processed_strings) == len(values)\n",
        "\n",
        "    # scale values\n",
        "    if max_value is None:\n",
        "        max_value = max(max(values), -min(values))+1e-3\n",
        "    scaled_values = [v / max_value * saturation for v in values]\n",
        "\n",
        "    # create html\n",
        "    html = \"\"\n",
        "    for i, s in enumerate(processed_strings):\n",
        "        if i<len(scaled_values):\n",
        "            v = scaled_values[i]\n",
        "        else:\n",
        "            v = 0\n",
        "        if v < 0:\n",
        "            hue = 0  # hue for red in HSV\n",
        "        else:\n",
        "            hue = 0.66  # hue for blue in HSV\n",
        "        rgb_color = colorsys.hsv_to_rgb(\n",
        "            hue, v, 1\n",
        "        )  # hsv color with hue 0.66 (blue), saturation as v, value 1\n",
        "        hex_color = \"#%02x%02x%02x\" % (\n",
        "            int(rgb_color[0] * 255),\n",
        "            int(rgb_color[1] * 255),\n",
        "            int(rgb_color[2] * 255),\n",
        "        )\n",
        "        html += f'<span style=\"background-color: {hex_color}; border: 1px solid lightgray; font-size: 16px; border-radius: 3px;\">{s}</span>'\n",
        "    if return_string:\n",
        "        return html\n",
        "    else:\n",
        "        display(HTML(html))\n",
        "\n",
        "def basic_feature_vis(encoder, text, feature_index, max_val=0):\n",
        "    feature_in = encoder.W_enc[:, feature_index]\n",
        "    feature_bias = encoder.b_enc[feature_index]\n",
        "    _, cache = model.run_with_cache(text, stop_at_layer=1, names_filter=utils.get_act_name(\"post\", 0))\n",
        "    mlp_acts = cache[utils.get_act_name(\"post\", 0)][0]\n",
        "    feature_acts = F.relu((mlp_acts - encoder.b_dec) @ feature_in + feature_bias)\n",
        "    if max_val==0:\n",
        "        max_val = max(1e-7, feature_acts.max().item())\n",
        "        # print(max_val)\n",
        "    # if min_val==0:\n",
        "    #     min_val = min(-1e-7, feature_acts.min().item())\n",
        "    return basic_token_vis_make_str(text, feature_acts, max_val)\n",
        "def basic_token_vis_make_str(model, strings, values, max_val=None):\n",
        "    if not isinstance(strings, list):\n",
        "        strings = model.to_str_tokens(strings)\n",
        "    values = utils.to_numpy(values)\n",
        "    if max_val is None:\n",
        "        max_val = values.max()\n",
        "    # if min_val is None:\n",
        "    #     min_val = values.min()\n",
        "    header_string = f\"<h4>Max Range <b>{values.max():.4f}</b> Min Range: <b>{values.min():.4f}</b></h4>\"\n",
        "    header_string += f\"<h4>Set Max Range <b>{max_val:.4f}</b></h4>\"\n",
        "    # values[values>0] = values[values>0]/ma|x_val\n",
        "    # values[values<0] = values[values<0]/abs(min_val)\n",
        "    body_string = create_html(strings, values, max_value=max_val, return_string=True)\n",
        "    return header_string + body_string\n",
        "# display(HTML(basic_token_vis_make_str(tokens[0, :10], mlp_acts[0, :10, 7], 0.1)))\n",
        "# # %%\n",
        "# The `with gr.Blocks() as demo:` syntax just creates a variable called demo containing all these components\n",
        "import gradio as gr\n",
        "try:\n",
        "    demos[0].close()\n",
        "except:\n",
        "    pass\n",
        "demos = [None]\n",
        "def make_feature_vis_gradio(model, feature_id, starting_text=None, batch=None, pos=None):\n",
        "    if starting_text is None:\n",
        "        starting_text = model.to_string(all_tokens[batch, 1:pos+1])\n",
        "    try:\n",
        "        demos[0].close()\n",
        "    except:\n",
        "        pass\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.HTML(value=f\"Hacky Interactive Neuroscope for gelu-1l\")\n",
        "        # The input elements\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                text = gr.Textbox(label=\"Text\", value=starting_text)\n",
        "                # Precision=0 makes it an int, otherwise it's a float\n",
        "                # Value sets the initial default value\n",
        "                feature_index = gr.Number(\n",
        "                    label=\"Feature Index\", value=feature_id, precision=0\n",
        "                )\n",
        "                # # If empty, these two map to None\n",
        "                max_val = gr.Number(label=\"Max Value\", value=None)\n",
        "                # min_val = gr.Number(label=\"Min Value\", value=None)\n",
        "                inputs = [text, feature_index, max_val]\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                # The output element\n",
        "                out = gr.HTML(label=\"Neuron Acts\", value=basic_feature_vis(starting_text, feature_id))\n",
        "        for inp in inputs:\n",
        "            inp.change(basic_feature_vis, inputs, out)\n",
        "    demo.launch(share=True)\n",
        "    demos[0] = demo"
      ],
      "metadata": {
        "id": "k2Cb-XmrVqgl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Function Def` - Inspecting Top Logits"
      ],
      "metadata": {
        "id": "O2UuRmEXVtBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SPACE = \"Â·\"\n",
        "NEWLINE=\"â†©\"\n",
        "TAB = \"â†’\"\n",
        "def process_token(s):\n",
        "    if isinstance(s, torch.Tensor):\n",
        "        s = s.item()\n",
        "    if isinstance(s, np.int64):\n",
        "        s = s.item()\n",
        "    if isinstance(s, int):\n",
        "        s = model.to_string(s)\n",
        "    s = s.replace(\" \", SPACE)\n",
        "    s = s.replace(\"\\n\", NEWLINE+\"\\n\")\n",
        "    s = s.replace(\"\\t\", TAB)\n",
        "    return s\n",
        "\n",
        "def process_tokens(l):\n",
        "    if isinstance(l, str):\n",
        "        l = model.to_str_tokens(l)\n",
        "    elif isinstance(l, torch.Tensor) and len(l.shape)>1:\n",
        "        l = l.squeeze(0)\n",
        "    return [process_token(s) for s in l]\n",
        "\n",
        "def process_tokens_index(l):\n",
        "    if isinstance(l, str):\n",
        "        l = model.to_str_tokens(l)\n",
        "    elif isinstance(l, torch.Tensor) and len(l.shape)>1:\n",
        "        l = l.squeeze(0)\n",
        "    return [f\"{process_token(s)}/{i}\" for i,s in enumerate(l)]\n",
        "\n",
        "def create_vocab_df(logit_vec, make_probs=False, full_vocab=None):\n",
        "    if full_vocab is None:\n",
        "        full_vocab = process_tokens(model.to_str_tokens(torch.arange(model.cfg.d_vocab)))\n",
        "    vocab_df = pd.DataFrame({\"token\": full_vocab, \"logit\": utils.to_numpy(logit_vec)})\n",
        "    if make_probs:\n",
        "        vocab_df[\"log_prob\"] = utils.to_numpy(logit_vec.log_softmax(dim=-1))\n",
        "        vocab_df[\"prob\"] = utils.to_numpy(logit_vec.softmax(dim=-1))\n",
        "    return vocab_df.sort_values(\"logit\", ascending=False)"
      ],
      "metadata": {
        "id": "-SBs3A3PVury"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.2.4 - `Function Defs` - Make Token DataFrame"
      ],
      "metadata": {
        "id": "zaT9C-f4VxgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_flatten(nested_list):\n",
        "    return [x for y in nested_list for x in y]\n",
        "def make_token_df(tokens, len_prefix=5, len_suffix=1):\n",
        "    str_tokens = [process_tokens(model.to_str_tokens(t)) for t in tokens]\n",
        "    unique_token = [[f\"{s}/{i}\" for i, s in enumerate(str_tok)] for str_tok in str_tokens]\n",
        "\n",
        "    context = []\n",
        "    batch = []\n",
        "    pos = []\n",
        "    label = []\n",
        "    for b in range(tokens.shape[0]):\n",
        "        # context.append([])\n",
        "        # batch.append([])\n",
        "        # pos.append([])\n",
        "        # label.append([])\n",
        "        for p in range(tokens.shape[1]):\n",
        "            prefix = \"\".join(str_tokens[b][max(0, p-len_prefix):p])\n",
        "            if p==tokens.shape[1]-1:\n",
        "                suffix = \"\"\n",
        "            else:\n",
        "                suffix = \"\".join(str_tokens[b][p+1:min(tokens.shape[1]-1, p+1+len_suffix)])\n",
        "            current = str_tokens[b][p]\n",
        "            context.append(f\"{prefix}|{current}|{suffix}\")\n",
        "            batch.append(b)\n",
        "            pos.append(p)\n",
        "            label.append(f\"{b}/{p}\")\n",
        "    # print(len(batch), len(pos), len(context), len(label))\n",
        "    return pd.DataFrame(dict(\n",
        "        str_tokens=list_flatten(str_tokens),\n",
        "        unique_token=list_flatten(unique_token),\n",
        "        context=context,\n",
        "        batch=batch,\n",
        "        pos=pos,\n",
        "        label=label,\n",
        "    ))"
      ],
      "metadata": {
        "id": "SEk5D7xwVy2D"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. `Implementation` - \"Standard Model\" ğŸ˜‰ Training and Evaluation"
      ],
      "metadata": {
        "id": "44TdMiajg-On"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 - Load the Models"
      ],
      "metadata": {
        "id": "WcPFxg3xgYBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(config=config.transformer)\n",
        "# transformer = TransformerWithAE(config=config.transformer)"
      ],
      "metadata": {
        "id": "15zw3V1ZgEUl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "01eb986d-ad75-42a0-a429-36b253dc078f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-7c8745c2f2d3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# transformer = TransformerWithAE(config=config.transformer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-858b3ec4a168>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, ae_hidden_dim, autoencoder)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_hidden_dim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# AE for higher-dimensional projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTransformerBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-94497a9293c1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, hidden_dim)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Encoder that projects to a higher-dimensional space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         self.encoder = nn.Sequential(\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Optional: Dropout for regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 - Train the model"
      ],
      "metadata": {
        "id": "WQ0ILsUtgi7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config.training.optimizer = torch.optim.AdamW(\n",
        "    params=model.parameters(),\n",
        "    lr=transformer.config.learning_rate\n",
        ")\n",
        "\n",
        "transformer, training_metrics, test_data = train(\n",
        "    model=transformer,\n",
        "    tokenizer=tokenizer,\n",
        "    data=data,\n",
        "    config=config.training\n",
        ")"
      ],
      "metadata": {
        "id": "etrUAZYIgifj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "d076a71f-32b2-453a-cc38-e8496d7d6bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "__main__.train() argument after ** must be a mapping, not TrainingConfig",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-b50b72fca327>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m config.training.optimizer = torch.optim.AdamW(\n\u001b[1;32m      2\u001b[0m     params=model.parameters(), lr=transformer.config.learning_rate)\n\u001b[0;32m----> 3\u001b[0;31m training_metrics, test_data = train(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __main__.train() argument after ** must be a mapping, not TrainingConfig"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 - Inspect training metrics and perform abbreviated evaluation"
      ],
      "metadata": {
        "id": "AXXqSB1H7TVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_args = {\n",
        "    # - [ ] TODO: Figure out what evaluation arguments are needed...\n",
        "}\n",
        "evaluate(model, test_data, **eval_args)"
      ],
      "metadata": {
        "id": "3otNK95_7Xq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. `Implementation` - Mechanistic Interpretability Analysis"
      ],
      "metadata": {
        "id": "EzyPR6QrV-U4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 - Using the Autoencoder\n"
      ],
      "metadata": {
        "id": "4OM8HO6OWH7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = get_recons_loss(num_batches=5, local_encoder=autoencoder)"
      ],
      "metadata": {
        "id": "2SXB4-mdWK60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yvxiv_DKWLeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 - Rare Features Are All The Same"
      ],
      "metadata": {
        "id": "x9Iok8SZB4fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each feature we can get the frequency at which it's non-zero (per token, averaged across a bunch of batches), and plot a histogram"
      ],
      "metadata": {
        "id": "skvpaPClM7Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = get_freqs(num_batches = 50, local_encoder = autoencoder)"
      ],
      "metadata": {
        "id": "5SECGnWLNAlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add 1e-6.5 so that dead features show up as log_freq -6.5\n",
        "log_freq = (freqs + 10**-6.5).log10()\n",
        "px.histogram(utils.to_numpy(log_freq), title=\"Log Frequency of Features\", histnorm='percent')"
      ],
      "metadata": {
        "id": "_Zn7g7WgNTQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that it's clearly bimodal! Let's define rare features as those with freq < 1e-4, and look at the cosine sim of each feature with the average rare feature - we see that almost all rare features correspond to this feature!"
      ],
      "metadata": {
        "id": "c612eWjYNpdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "is_rare = freqs < 1e-4\n",
        "rare_enc = autoencoder.W_enc[:, is_rare]\n",
        "rare_mean = rare_enc.mean(-1)\n",
        "px.histogram(utils.to_numpy(rare_mean @ autoencoder.W_enc / rare_mean.norm() / autoencoder.W_enc.norm(dim=0)), title=\"Cosine Sim with Ave Rare Feature\", color=utils.to_numpy(is_rare), labels={\"color\": \"is_rare\", \"count\": \"percent\", \"value\": \"cosine_sim\"}, marginal=\"box\", histnorm=\"percent\", barmode='overlay')"
      ],
      "metadata": {
        "id": "GbM9UZsJN0Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 - Interpreting A Feature"
      ],
      "metadata": {
        "id": "eZCqUIB9Bzx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go and investigate a non rare feature, feature 7"
      ],
      "metadata": {
        "id": "3m_u8-zXRW2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_id = 7 # @param {type:\"number\"}\n",
        "batch_size = 128 # @param {type:\"number\"}\n",
        "\n",
        "print(f\"Feature freq: {freqs[7].item():.4f}\")"
      ],
      "metadata": {
        "id": "_HrsTqVIUKC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run the model on some text and then use the autoencoder to process the MLP activations"
      ],
      "metadata": {
        "id": "AYoGLyl2VpA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = all_tokens[:batch_size]\n",
        "_, cache = model.run_with_cache(tokens, stop_at_layer=1, names_filter=utils.get_act_name(\"post\", 0))\n",
        "mlp_acts = cache[utils.get_act_name(\"post\", 0)]\n",
        "mlp_acts_flattened = mlp_acts.reshape(-1, config.autoencoder.d_mlp)\n",
        "loss, x_reconstruct, hidden_acts, l2_loss, l1_loss = autoencoder(mlp_acts_flattened)\n",
        "# This is equivalent to:\n",
        "# hidden_acts = F.relu((mlp_acts_flattened - encoder.b_dec) @ encoder.W_enc + encoder.b_enc)\n",
        "print(\"hidden_acts.shape\", hidden_acts.shape)"
      ],
      "metadata": {
        "id": "TBKY9P_PULJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now sort and display the top tokens, and we see that this feature activates on text like \" and I\" (ditto for other connectives and pronouns)! It seems interpretable!\n",
        "\n",
        "**Aside:** Note on how to read the context column:\n",
        "\n",
        "A line like \"Â·himselfÂ·asÂ·democraticÂ·socialistÂ·and|Â·he|Â·favors\" means that the preceding 5 tokens are \" himself as democratic socialist and\", the current token is \" he\" and the next token is \" favors\".  Â· are spaces, â†© is a newline.\n",
        "\n",
        "This gets a bit confusing for this feature, since the pipe separators look a lot like a capital I\n"
      ],
      "metadata": {
        "id": "D0-vxYAIVtxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_df = make_token_df(tokens)\n",
        "token_df[\"feature\"] = utils.to_numpy(hidden_acts[:, feature_id])\n",
        "token_df.sort_values(\"feature\", ascending=False).head(20).style.background_gradient(\"coolwarm\")"
      ],
      "metadata": {
        "id": "6LQet6vaU60H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's easy to misread evidence like the above, so it's useful to take some text and edit it and see how this changes the model's activations. Here's a hacky interactive tool to play around with some text."
      ],
      "metadata": {
        "id": "H_SsYbIhWm43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.cfg"
      ],
      "metadata": {
        "id": "xHWCNBe81qAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"The 1899 Kentucky gubernatorial election was held on November 7, 1899. The Republican incumbent, William Bradley, was term-limited. The Democrats chose William Goebel. Republicans nominated William Taylor. Taylor won by a vote of 193,714 to 191,331. The vote was challenged on grounds of voter fraud, but the Board of Elections, though stocked with pro-Goebel members, certified the result. Democratic legislators began investigations, but before their committee could report, Goebel was shot by an unknown assassin (event pictured) on January 30, 1900. Democrats voided enough votes to swing the election to Goebel, Taylor was deposed, and Goebel was sworn into office on January 31. He died on February 3. The lieutenant governor of Kentucky, J. C. W. Beckham, became governor, and battled Taylor in court. Beckham won on appeal, and Taylor fled to Indiana, fearing arrest as an accomplice. The only persons convicted in connection with the killing were later pardoned; the assassin's identity remains a mystery\"\n",
        "t = model.to_tokens(s)\n",
        "print(t)"
      ],
      "metadata": {
        "id": "wx6MkfEk0osY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "starting_text = \"Hero and I will head to Samantha and Mark's, then he and she will. Then I or you\" # @param {type:\"string\"}\n",
        "make_feature_vis_gradio(feature_id, starting_text)"
      ],
      "metadata": {
        "id": "HbgjmwTnQman"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. `Conjecture` - Connections with Supersymmetry"
      ],
      "metadata": {
        "id": "vxzEg4XchFe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 - Notation"
      ],
      "metadata": {
        "id": "_mhMjXIZw3Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d1qM2PckhMXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 - Motivation"
      ],
      "metadata": {
        "id": "6Kow525Uw57l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Ni57IZlw_Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Formalism"
      ],
      "metadata": {
        "id": "u_mCqkVVxBsw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m2T4BRqoxFDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. `Conjecture` - Next Steps"
      ],
      "metadata": {
        "id": "Y8o7zwIHxQUw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HuxzKQeexRrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. References"
      ],
      "metadata": {
        "id": "L759zICsht0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NZaMtzWhhw_B"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}